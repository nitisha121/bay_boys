---
title: "STAT 331 Final Report"
author: "Nitisha Agarwal and Maya Perelman"
date: "12/1/2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, echo = TRUE)
library("Amelia")
library("mice")
```

## I. Summary

The goal of this report is to explore the determinants of healthy male birth weight in a sample population collected from a 1960s CHDS survey in San Francisco. The dataset contains 1236 observations of healthy male single-fetus births over 18 variates. Multiple linear regression allowed connections to be made regarding the independent variables’ effects on the dependent variable of healthy birth weight.
 
Unsurprisingly, the dataset had missing values and proper treatment of these NAs was carried out through a variety of methods, including MICE. The complete datasets obtained from imputation led to the development of about 15 models. In order to select the best candidate models, diagnostics such as cross validation and residual analysis were employed. 
 
In the end, the selected model was a modified version of the one produced by automatic stepwise selection. During initial model diagnostics, it was observed that this model had the lowest residual standard errors, AIC, PRESS statistic scores, and the lowest rPMSE obtained through cross validation. It was confirmed that the stepwise model did not violate any of the assumptions of linear regression. To obtain the final model, this stepwise model was tweaked to add and remove interactions as deemed necessary.  

## II. Model Selection 

### Pre-Fitting Data Diagnostics

In order to do some initial data exploration, we took a look at pairs plots, summary of the data set, and dependant variable distribution. The noteworthy points are presented below. 

```{r,echo=FALSE, fig.height=4.1, fig.width=7.5}
# Our data set and MICE Imputation

# setwd("/Users/mayaperelman/Documents/STAT_331/FinalProject/")
chdbirths <- read.csv("C:/My data/university/third year/stat331/final_proj/chds_births.csv")

#Pairs plots, adjust which variables you want in it
pairs(chdbirths[,c(1,2,3,14,8)],pch=16, cex=.3, col=adjustcolor("black",0.5))

```
$\textbf{Figure 1}$: Pairs plot across select variables.


We can make several observations from Figure 1. The values for gestation are very concentrated between 250-300 which is expected for normal human gestation period. From the data collected for the marital covariate, we see a distinct line at marital = 1 which tells us that a large majority of moms are married. We also see that there are 2 observations encoded as marital = 0, despite 0 not being an option for marital status. This is a consideration that we addressed when re-coding our data. Moreover, we notice that high values of parity (>4) are uncommon, which is unsurprising.

 
Our dependent variable of healthy male birth weight should have a normal distribution, as the average weight of a healthy baby is known. Let's verify this claim with a histogram.

```{r,fig.height=2.5,fig.width=4, echo=FALSE}
hist(chdbirths$wt,freq=FALSE, breaks="FD", 
     col="gray90",
     xlab = "Healthy Male Birth Weight",
     main = "Histogram of Birth Weight")
curve(dnorm(x,mean=mean(chdbirths$wt),sd=sd(chdbirths$wt)),col="red",add=TRUE)
```
  
$\textbf{Figure 2}$: Distribution of dependent variable.

As predicted, the distribution of birth weight is in fact normally distributed and well approximated by the Gaussian distribution as shown in Figure 2.  
 
```{r,echo=FALSE}
births <- chdbirths
# Re-coding the data into factor variables
#Ethnicity
births$meth[chdbirths$meth %in% c(0,1,2,3,4,5)] <- "Caucasian"
births$meth[chdbirths$meth == 6] <- "Mexican"
births$meth[chdbirths$meth == 7] <- "AA"
births$meth[chdbirths$meth == 8] <- "Asian"
births$meth[chdbirths$meth %in% c(9,10)] <- "Other" # grouping in Mixed 

births$feth[chdbirths$feth %in% c(0,1,2,3,4,5)] <- "Caucasian"
births$feth[chdbirths$feth == 6] <- "Mexican"
births$feth[chdbirths$feth == 7] <- "AA"
births$feth[chdbirths$feth == 8] <- "Asian"
births$feth[chdbirths$feth %in% c(9,10)] <- "Other" # grouping in Mixed 


# Education
births$med[chdbirths$med == 0] <- "elem"
births$med[chdbirths$med == 1] <- "mid"
births$med[chdbirths$med == 2] <- "hs"
births$med[chdbirths$med == 3] <- "hs+trade"
births$med[chdbirths$med == 4] <- "hs+some+col"
births$med[chdbirths$med == 5] <- "col"
births$med[chdbirths$med == 6] <- "trade"
births$med[chdbirths$med == 7] <- "hs+unclear"

births$fed[chdbirths$fed == 0] <- "elem"
births$fed[chdbirths$fed == 1] <- "mid"
births$fed[chdbirths$fed == 2] <- "hs"
births$fed[chdbirths$fed == 3] <- "hs+trade"
births$fed[chdbirths$fed == 4] <- "hs+some+col"
births$fed[chdbirths$fed == 5] <- "col"
births$fed[chdbirths$fed == 6] <- "trade"
births$fed[chdbirths$fed == 7] <- "hs+unclear"

# Marital Status
births$marital[chdbirths$marital == 1] <- "married"
# Grouping all other types because vast majority of obs. are married
births$marital[chdbirths$marital %in% c(0,2,3,4,5)] <- "other"


# Smoke
births$smoke[chdbirths$smoke == 0] <- "never"
births$smoke[chdbirths$smoke == 1] <- "smokesnow"
births$smoke[chdbirths$smoke == 2] <- "untilpreg"
births$smoke[chdbirths$smoke == 3] <- "usedto"

#Time, changing order
births$time[chdbirths$time == 1] <- 8
births$time[chdbirths$time == 2] <- 7
births$time[chdbirths$time == 3] <- 6
births$time[chdbirths$time == 4] <- 5
births$time[chdbirths$time == 5] <- 4
births$time[chdbirths$time == 6] <- 3
births$time[chdbirths$time == 7] <- 2
births$time[chdbirths$time == 8] <- 1
births$time[chdbirths$time == 9] <- NA # only 5 observations, will be deleted 

```
### Treatment of NA's

We run an NA count on our `chdbirths` data set to determine how to proceed with treatment of missing data.  

```{r,echo=FALSE}
# Counting NA's in each variate
NAcount <- sapply(chdbirths, function(x) sum(is.na(x)))
NAcount
```

$\;$  
Immediately, we notice that father weight and height are missing almost 40% of values, so we will have to discard these variables from our analysis. After doing so, we consider a visual map of the distribution of NAs (yellow) among our data set using the `Amelia` package in R. Fun fact: the Amelia package in R is used for treatment of missing data, and named after Amelia Earhart. 

$\;$

```{r, error=TRUE, echo=FALSE, fig.height = 3.5}
options(warn=-1)
# Dropping father height and weight since so many NA's 
drops <- c("fwt","fht")
births <- births[,!(names(births) %in% drops)]

missmap(births, col=c("yellow","midnightblue"),y.labels="",y.at="", cex.axis = 0.7)
```
  
$\textbf{Figure 3}$: Missingness map for NAs by covariate.

As we can see there are a significant amount of NA's in the income covariate, around 10% of the observations, and we will want to impute this in order to not lose a valuable predictor. 

We will use the R `mice` package to do multiple imputation on our missing values in the remaining continous variables (refer to Appendix A). In particular, the imputation on income is important since around 10% of its values are missing, and removing these could bias our analysis. For the remaining NAs in categorical variables we simply remove those observations. As a result, we only remove 55 observations, which is around 6% of our data set. 

```{r, results='hide',echo=FALSE}
options(warn=-1)
#removing observations with NAs in variables that aren't income
birthsCompl <- births[complete.cases(births$marital, births$meth,
                                     births$med,births$feth,
                                     births$fed, births$smoke, births$time, births$number),]
# using the mice function to impute missing values over 5 iterations
set.seed(300)
imp_all <- mice(birthsCompl,m=5)
```

```{r,fig.align='center',fig.height=3.7,fig.width=6.5, echo=FALSE}
options(warn=-1)
densityplot(imp_all,cex.axis=0.8) # displaying differences between observations and imputed data
```
$\textbf{Figure 4}$: Plot to display difference between imputed values (pink) and observed (blue).

These density plots in Figure 4 show the distribution of the data of existing observations (blue) compared to the distribution of the imputed values (pink) for each iteration of the MICE algorithm. As we can see, the imputation of father age varies greatly because there are only 7 missing values, creating variability. The other distributions of imputed values seem to be consistent with the existing data. 


### Automatic Model Selection

```{r,echo=FALSE}

# Stepwise Model Selection
for (i in 1:5){
  compldata <- complete(imp_all,i) # Completing data set over each imputation
  M0 <- lm(wt~1,data=compldata) # minimal model: intercept only
  
  # all main effects and interactions
  ##  Removed feth, meth, fed, med interactions because they produced NA's
  Mmax <- lm(wt ~ (.-feth - meth - fed - med)^2 + feth + meth + fed + med,data=compldata)
  Mstart <-lm(wt ~., data = compldata) # starting model for stepwise
  Mstep <- step(object= Mstart,
              scope=list(lower=M0, upper = Mmax),
              direction ="both",
              trace=FALSE)
  model <- paste("Ms",i, sep="") # dynamically name models with corresponding index
  assign(model,Mstep)
}

# Forward Model Selection
for (i in 1:5){
  compldata <- complete(imp_all,i) # Completing data set over each imputation
  Mfwd <- step(object = M0,
              scope=list(lower=M0, upper = Mmax),
              direction ="forward",
              trace=FALSE)
  model <- paste("Mf",i, sep="") # dynamically name models with corresponding index
  assign(model,Mfwd)
}

# Backwards Model Selection
for (i in 1:5){
  compldata <- complete(imp_all,i) # Completing data set over each imputation
  Mback <- step(object = Mmax,
              scope=list(lower=M0, upper = Mmax),
              direction ="backward",
              trace=FALSE)
  model <- paste("Mb",i, sep="") # dynamically name models with corresponding index
  assign(model,Mback)
}

for (i in 1:5){
  compldata <- complete(imp_all,i)[-246,] # Completing data set over each imputation
  M0.out <- lm(wt~1,data=compldata) # minimal model: intercept only
  
  # all main effects and interactions
  ##  Removed feth, meth, fed, med interactions because they produced NA's
  Mmax.out <- lm(wt ~ (.-feth - meth - fed - med)^2 + feth + meth + fed + med,data=compldata)
  Mstart.out <-lm(wt ~., data = compldata) # starting model for stepwise
  Mstep1 <- step(object= Mstart.out,
              scope=list(lower=M0.out, upper = Mmax.out),
              direction ="both",
              trace=FALSE)
  model <- paste("MsOut",i, sep="") # dynamically name models with corresponding index
  assign(model,Mstep1)
}

```

We will now run the 3 automated model selection algorithms over each of our 5 imputed data sets. This may generate up to 15 different models (some may be the same formula). Refer to *Appendix B* for the code used to run automated model selection. In order to narrow down to the best candidate models, we considered 3 main metrics of model quality, summarized in the table below. 

### Manual Model Selection  

```{r,echo=FALSE}

pressc1 <- sum((resid(Ms1)/(1-hatvalues(Ms1)))^2)
pressc2 <- sum((resid(Ms2)/(1-hatvalues(Ms2)))^2)
pressc3 <- sum((resid(Ms3)/(1-hatvalues(Ms3)))^2)
pressc4 <- sum((resid(Ms4)/(1-hatvalues(Ms4)))^2)
pressc5 <- sum((resid(Ms5)/(1-hatvalues(Ms5)))^2)

pressf1 <- sum((resid(Mf1)/(1-hatvalues(Mf1)))^2)
pressf2 <- sum((resid(Mf2)/(1-hatvalues(Mf2)))^2)
pressf3 <- sum((resid(Mf3)/(1-hatvalues(Mf3)))^2)
pressf4 <- sum((resid(Mf4)/(1-hatvalues(Mf4)))^2)
pressf5 <- sum((resid(Mf5)/(1-hatvalues(Mf5)))^2)

pressb1 <- sum((resid(Mb1)/(1-hatvalues(Mb1)))^2)
pressb2 <- sum((resid(Mb2)/(1-hatvalues(Mb2)))^2)
pressb3 <- sum((resid(Mb3)/(1-hatvalues(Mb3)))^2)
pressb4 <- sum((resid(Mb4)/(1-hatvalues(Mb4)))^2)
pressb5 <- sum((resid(Mb5)/(1-hatvalues(Mb5)))^2)

```


```{r echo =FALSE}
# Stepwise models summary 

step.matrix <- matrix(0, 3, 5)
rownames(step.matrix) <- c("AIC", "RSE", "PRESS")
colnames(step.matrix) <- c("Ms1", "Ms2", "Ms3", "Ms4", "Ms5")

aic.s <- round(c(AIC(Ms1), AIC(Ms2), AIC(Ms3), AIC(Ms4), AIC(Ms5)),2)
rse.s <- round(c(sigma(Ms1), sigma(Ms2), sigma(Ms3), sigma(Ms4), sigma(Ms5)),2)
press.s <- round(c(pressc1, pressc2, pressc3, pressc4, pressc5),2)

step.matrix[1,] <- aic.s
step.matrix[2,] <- rse.s
step.matrix[3,] <- press.s

step.matrix
```
$\textbf{Table 1}$: Summary statistics for stepwise selection models over 5 imputed data sets.  

Based off of these metrics, the model produced by Stepwise selection on the 4th imputed data set had the lowest AIC, RSE, and PRESS, which indicates that it's a better model fit. We will proceed with this one to our cross-validation step.  Next, here is the summary table for our *Forward selection* models. 
```{r,echo=FALSE}
# Forward models summary 

for.matrix <- matrix(0, 3, 5)
rownames(for.matrix) <- c("AIC", "RSE", "PRESS")
colnames(for.matrix) <- c("Mf1", "Mf2", "Mf3", "Mf4", "Mf5")

aic.f <- round(c(AIC(Mf1), AIC(Mf2), AIC(Mf3), AIC(Mf4), AIC(Mf5)),2)
rse.f <- round(c(sigma(Mf1), sigma(Mf2), sigma(Mf3), sigma(Mf4), sigma(Mf5)),2)
press.f <- round(c(pressf1, pressf2, pressf3, pressf4, pressf5),2)

for.matrix[1,] <- aic.f
for.matrix[2,] <- rse.f
for.matrix[3,] <- press.f

for.matrix

```
$\textbf{Table 2}$: Summary statistics for forward selection models over 5 imputed data sets. 

We can observe that the 4th model has the lowest RSE, AIC, and PRESS statistics scores. As such, we will proceed with 4th forward model to the cross-validation step.  Next, we have the summary table for our *Backward selection* models.  
```{r, echo=FALSE}
# Backwards models summary 
back.matrix <- matrix(0, 3, 5)
rownames(back.matrix) <- c("AIC", "RSE", "PRESS")
colnames(back.matrix) <- c("Mb1", "Mb2", "Mb3", "Mb4", "Mb5")

aic.b <- round(c(AIC(Mb1), AIC(Mb2), AIC(Mb3), AIC(Mb4), AIC(Mb5)),2)
rse.b <- round(c(sigma(Mb1), sigma(Mb2), sigma(Mb3), sigma(Mb4), sigma(Mb5)),2)
press.b <- round(c(pressb1, pressb2, pressb3, pressb4, pressb5),2)

back.matrix[1,] <- aic.b
back.matrix[2,] <- rse.b
back.matrix[3,] <- press.b

back.matrix
```
$\textbf{Table 3}$: Summary statistics for backward elimination models over 5 imputed data sets. 

Based off of these metrics, the model produced by backward elimination on the 4th imputed data set had the lowest AIC, RSE, and PRESS, which indicates that it's a better model fit. However, the model call is very convoluted and includes a multitude of interactions that are hard to justify logically.As such, we proceed with our forward and stepwise models to cross-validation.


### Cross-Validation

We will run cross-validation on the 2 candidate models `Ms4`, and `Mf4`.
The resulting boxplot of RPMSE and histogram of logLambda are shown below.   

```{r,include=FALSE}
fit.step <- with(data=imp_all, exp = lm(formula = wt ~ gestation + parity + meth + mage + mht    + mwt + marital + income + smoke + time + number + gestation:income + 
    gestation:number + parity:marital + mwt:income + mht:marital + 
    gestation:mage + gestation:mwt + gestation:mht + mage:time + 
    mage:number + smoke:number))


fit.fwd <- with(data=imp_all, lm(formula = wt ~ gestation + smoke + mht + meth + parity + number + mwt + time + gestation:number + gestation:mwt + gestation:mht))

fit.stepOut <- with(data=imp_all,exp=lm(formula = wt ~ gestation + parity + meth + mage + mht + mwt + 
    marital + income + smoke + time + number + gestation:income + 
    gestation:number + mwt:income + gestation:mage + parity:marital + 
    mht:marital + gestation:mwt + gestation:mht + time:number))
```

```{r,echo=FALSE,fig.height=3.7}
options(warn=-1)
# models to compare are stepwise and forward

set.seed(331)
 # Cross-validation setup
 nreps <- 1000 # number of replications
 ntot <- nrow(birthsCompl) # total number of observations
 ntrain <- 0.8*ntot # size of training set is 80% of total 
 ntest <- ntot - ntrain # size of test set is 20% of total 
 mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
 mspe2 <- rep(NA, nreps)

 logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
 m <-5 # number of imputations
 

  for (ii in 1:nreps) {
    
    # randomly select training observations
    sample.ind <- sample(m,1) # randomly select which imputed data set to use
    
    M1.model <- fit.fwd$analyses[[sample.ind]]
    M2.model <- fit.step$analyses[[sample.ind]]
    train.ind <- sample(ntot, ntrain) # training observations
    # refit the models on the given imputed data and training indices 
    data_complete <- complete(imp_all,sample.ind)
    M1.cv <- update(M1.model, data = data_complete[train.ind,])
    M2.cv <- update(M2.model, data = data_complete[train.ind,])
    # out-of-sample residuals for both models
    # that is, testing data - predictions with training parameters
    M1.res <- data_complete$wt[-train.ind] -
      predict(M1.cv, newdata = data_complete[-train.ind, ])
    M2.res <- data_complete$wt[-train.ind] -
      predict(M2.cv, newdata = data_complete[-train.ind, ])
    # mean-square prediction errors
    mspe1[ii] <- mean(M1.res ^ 2)
    mspe2[ii] <- mean(M2.res ^ 2)
    # out-of-sample likelihood ratio
    M1.sigma <- sqrt(sum(resid(M1.cv) ^ 2) / ntrain) # MLE of sigma
    M2.sigma <- sqrt(sum(resid(M2.cv) ^ 2) / ntrain)
    # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
    logLambda[ii] <-
      sum(dnorm(
        M1.res,
        mean = 0,
        sd = M1.sigma,
        log = TRUE
      ))
    logLambda[ii] <- logLambda[ii] -
      sum(dnorm(
        M2.res,
        mean = 0,
        sd = M2.sigma,
        log = TRUE
      ))
  }

Mnames <- expression(M[FWD],M[STEP])

par(mfrow=c(1,2))
boxplot(list(sqrt(mspe1),sqrt(mspe2)), names = Mnames, ylab = expression(sqrt(MSPE)), cex = 0.7, col= c("springgreen4","skyblue"), cex.axis = 0.7, main = "RMSPE")

hist(logLambda, breaks = 30, freq = FALSE, xlab = expression(Lambda^{test}), cex=0.7,
     main = "log of Lambda", col="gray93", cex.axis = 0.7)
abline(v=mean(logLambda),col="red",lwd=2)


```
$\textbf{Figure 5}$: Boxplot (left) and histogram(right) of results of cross-validation.  

From the boxplot above we see that the stepwise model is comparitively the best one. The mean red line on the logLambda histogram is negative, indicating a preference for the stepwise model. Likewise, form hte boxplot we observe a preference for the stepwise model as well. 


## III. Model Diagnostics

We now compare and contrast the residuals of each model to make our final choice for the best candidate between stepwise and forward selection. 

First, let's take a look at each model's residual plots.  

```{r,echo=FALSE, fig.height=3.85}

im <- 5 # looking at 5th imputation 

# Residuals for stepwise model
res.s <- resid(fit.step$analyses[[im]])
h.s <- hatvalues(fit.step$analyses[[im]])
res.stu.s <- res.s/sqrt(1-h.s) # studentized residuals

# Residuals for forward model
res.f <- resid(fit.fwd$analyses[[im]])
h.f <- hatvalues(fit.fwd$analyses[[im]])
res.stu.f <- res.f/sqrt(1-h.f) # studentized residuals



# Plotting residual plots for stepwise model
cex <- 0.6
par(mfrow=c(2,2),mar=3*c(1,1,0.7,0.8))
plot(fit.step$analyses[[im]],cex=cex,col=adjustcolor("skyblue4",0.5),
     cex.axis=cex, pch=16, cex.lab=cex, cex.main=cex, mgp=c(2,1,0))

```
$\textbf{Figure 6}$: Residual plots for stepwise model.  

From these 4 diagnostic plots we can observe that there is no clear violation of the 3 assumptions of linear regression for the stepwise model. The residuals vs. fitted plot depicts an overall linear conditional mean, with randomly scattered residuals about the x=0 horizontal line. We observe a slight deviation of the red mean (of residual values at each fitted value) line from x=0 for higher fitted values, but since so few observations fall past a fitted value of 150, this is not too concerning. 

In the QQ-plot we notice a slight deviation at the tails, but overall the residuals of the stepwise model appear Normally distributed. The Scale-Location plot shows us that the assumption of equal variance (homoscedasticity) holds for this model, with a slight (but not significant) deviation for higher fitted values. Finally, the Residuals vs. Leverage plot shows us that there are several points with high leverage in the stepwise model, but none of them have a concerning influence (if they were outside the dotted red lines depicting Cook's distance). 

We will now consider these same plots for the *forward selection model*.

```{r,echo=FALSE, fig.height=4}
#Plotting residual plots for forward model
cex <- 0.6
par(mfrow=c(2,2),mar=3*c(1,1,0.7,0.8))
plot(fit.fwd$analyses[[im]],cex=cex,col=adjustcolor("midnightblue",0.5),
     cex.axis=cex, pch=16, cex.lab=cex, cex.main=cex, mgp=c(2,1,0))
```
$\textbf{Figure 7}$: Residual plots for forward model.  

Overall, the residual diagnostic plots look very similar to those for the stepwise model. We will highlight a couple of differences.

First, the red mean line in the Residuals vs. Fitted appears a little bit straighter, indicating that the linear conditional mean assumption holds a bit better for the forward model. The QQ-plot and Scale-Location plot have little to no differences to those for the stepwise model. The Residuals vs. Leverage plot once again shows only points with high leverage, and no particularly influential points. As in an ideal scenario, we can not even see the Cook's distance lines in the graph.   

The next thing we can take a look at are histograms of each model's studentized residuals.  

```{r,echo=FALSE,fig.height=3.2,fig.width=7}

# Plotting standardized residuals to see distribution
par(mfrow=c(1,2))
sigma.hat.s <- sigma(fit.step$ana[[im]])
sigma.hat.f <- sigma(fit.fwd$ana[[im]])

# Stepwise model studentized residual histogram
hist(res.stu.s/sigma.hat.s,breaks=40,freq=FALSE,cex.axis=0.8, 
     main = "Stepwise Model", xlab= "Studentized Residuals, standardized")
curve(dnorm(x),col="red",add=TRUE) # normal dist overlay

# Forward model studentized residual histogram
hist(res.stu.f/sigma.hat.f,breaks=40,freq=FALSE,cex.axis=0.8, 
     main = "Forward Model", xlab= "Studentized Residuals, standardized")
curve(dnorm(x),col="red",add=TRUE) # normal dist overlay

```
$\textbf{Figure 8}$: Histogram of studentized residuals for both models.

Once again, we see little to no differences between each model's residuals, now studentized and standardized. Both distributions are very slightly left skewed, supporting our previous observation of the residuals being somewhat negatively biased. Both also have slight peaks at the mean of 0 but are still well approximated by a Gaussian distribution. So we can say that both models do not violate the 6th assumption of regression, which claims iid Normal errors. 


Now let's take a look at the influence vs. leverage plot for both of our models. Below, we plot the stepwise model in red and forwards in blue.  


```{r, echo=FALSE, fig.width=5.5,fig.height=3.5, fig.align='center'}
D.step <- cooks.distance(fit.step$analyses[[im]])
D.fwd <- cooks.distance(fit.fwd$analyses[[im]])
h.step <- hatvalues(fit.step$analyses[[im]])
h.fwd <- hatvalues(fit.fwd$analyses[[im]])

inf <- which.max(D.step)
lev <- which.max(h.step)


plot(h.step,D.step, 
     col=adjustcolor("firebrick",0.5),
     pch=16, main = "Influence vs. Leverage Plot",
     xlab = "Leverage", ylab= "Influence (Cook's Distance)",
     cex.axis=0.8, mgp = c(2, 1, 0), cex.lab=0.7)
points(h.fwd,D.fwd,col=adjustcolor("navyblue",0.5),pch=18)
legend("topright", legend = c("Stepwise", "Forward"), pch = c(16,18),
       col = c("firebrick","navyblue"))
text(x=h.step[inf], y = D.step[inf],
     labels = inf, pos=1, cex = 0.7, offs=0.4)
text(x=h.step[lev], y = D.step[lev],
     labels = lev, pos=c(1,1,1), cex = 0.7, offs=0.4)

```
  
$\textbf{Figure 9}$: Influence vs. Leverage plot for both models. 

We can see that the stepwise model and the forwards model have a majority of the points in the same area of the graph, with a few slight exceptions. The stepwise model contains a few distinct high leverage points (labelled on the right) and a few high influence points (labelled near the top). Let's take a look at which observations these are to try and deduce why they may have high leverage/influence. 

```{r,echo=FALSE}
# Pulling out highest influence point
birthsCompl[inf,c(1,2,3,4,5,12,13,14)]
```

The highest influence point is an observation with one of the highest birth weights, but a relatively normal gestational period. We take a look at the other observations with equal or higher birth weight.  

```{r,echo=FALSE}
birthsCompl[birthsCompl$wt >= 174,c(1,2,3,4,5,12,13,14)]
```

We notice that other babies born with such a high birth weight were all born to Caucasian, married parents, unlike the highest influence point. Moreover, our observation had an income level of 1, which is the lowest of this group. These are the likely causes for the high influence of this observation.  


Now looking at the highest leverage point.
```{r,echo=FALSE}
#Pulling out highest leverage point
complete(imp_all,5)[lev,]

```

Looking at the highest leverage point, it is noticeable that observation 246 has a normal birth weight (mean birth weight is 119 ounces), and yet a very low gestational period (of about 21 weeks). From research on typical fetus weight gain during pregnancy, a fetus at 21 weeks is typically *12 ounces*, while this observation has a weight of *116 ounces*. This strikes us as a possible input error or improper identification of conception date on the part of the parents, that may have biased our automatic model selection.  

After looking at various model diagnostic measures, we have noticed very little differences between our two candidate models in terms of how well the linear regression assumptions hold. From our earlier analysis using cross validation, AIC, and PRESS statistics, the stepwise model was superior to our forward model on all 3 counts. As such, as our final model, we will select the one produced by *stepwise selection*.  

As discussed above however, our stepwise model may have been influenced by the high leverage point. After re-running automatic stepwise selection on all 5 imputed data sets *with observation 246 excluded*, we noticed that our optimal stepwise model call changed (refer to Appendix for details). The new stepwise model has a simpler formula with less terms, and most notably, now includes the `time:number` interaction. From our research, the metric "pack-years" which counts how much and for how long an individual smoked is quite influential on birth weight, and we believe that the `time:number` interaction is the best proxy for this measure.  

Running cross-validation across all 5 *complete* imputed data sets for the old and new stepwise selection models revealead that they had no significant difference in predictive power. As such, we will use the new stepwise model as our final model since it retains the predicitve power of the first one while being a simpler model. Refer to our Discussion section below for a more detailed explanation on why we felt it was justified to exclude observation 246.

As a final step, we pool the coefficients from the five imputations for our stepwise model, and display the results including the final model's call.  
```{r,echo=FALSE}
final.model <- pool(fit.stepOut)
MsOut4$call
round(summary(final.model),3)

```


### Errors and Trouble Encountered  

During our data analysis, we learned a lot about strategies to choose a well fitting model for a given sample population. In the end we retained a model that made sense to us, but to get there we did first run into a few hurdles. For example, when working with models with quite a few terms, we received an error of “Prediction from a rank-deficient fit may be misleading” which our research explained by claiming that as you add terms to a formula, regression will consider having too many predictors as a problem. To deal with this, we tried taking out a few interactions that weren’t intuitive and re-running cross-validation to see if the removal helped. Another issue we encountered was that interactions between variables such as parent's education and ethnicity which produced a lot of NA’s so we decided to restrict our models from including such interactions.  




## IV. Discussion 

1. From our analysis, we have explored the relation of the 17 independent variables given in our data set to healthy singleton male births in San Francisco in the 1960's. From our models, we have noticed that the most significant predictors for our given population included both physical factors and socioeconomic factors. In terms of physical attributes, length of gestational period was particularly predictive, as well as mother's height/weight and parity (whether the child was first born or not). In addition, the smoking status of the mother (whether/how much they smoked during pregnancy) as well as ethnicity (Caucasian tended to have higher birth weights) were noticed to be predictive. In terms of socioeconomic factors, income was a predictive variable in our models, with higher-income units typically having higher birth weights. However, we cannot make any conclusion about marital status based off of our model since 98% of our observations were married couples.  

2. Based off of our analysis, the biggest behavioural changes that could be made in order to avoid low birth weight would be to quit smoking as soon as possible, whether that be before pregnancy or as soon as pregnancy is known. As well, potentially ensuring that the mother has a healthy weight, and in particular is not significantly underweight, could help lower the risk of a low birth weight. As well, although not in our final model, from our research we haven noted that studies have found a strong association between illiteracy and low birth weight. This is another factor that could potentially be in control of the mother, depending on her socioeconomic resources.    

3. Yes, in our final model we retained coefficients with high p-values (>0.05). This was because the basis for adding/removing terms in the automatic stepwise selection algorithm was carried out based on AIC score, and not on p-value. As such, the algorithm optimized a different measure of statiscial significance, and it is still appropriate to keep these coefficients in our final model.    

4. Yes, as previously discussed when analyzing leverage above, observation 246 had an almost impossibly high birth weight given that the gestational period was around 21 weeks. This observation be appropriate to remove because even it if wasn't caused by incorrect measurement, is statistically very improbably to be repeated, and we wouldn't necessarily want a model to be influenced by it. As well, we noticed that there were quite a few observations with unusually high gestational periods. In particular, observations 11 and 1104 had gestational periods over 350 days, which is almost a year in length. Although removing these observations would likely not be appropriate, this makes us question exactly how gestational period was defined, and how likely the mother was to provide an accurate conception date. This might be an important consideration for any future studies conducted on this topic.  

5. As discussed in the model selection above, all of the assumptions of linear regression can be said to hold for our final model. Possible deficiences of our model is that it was trained on a very specific subpopulation. In the model diagnostics section above, we saw that both models failed to providence any evidence of a violation of regression assumptions. A possible deficiency of our model is that it was trained on a very specific subpopulation from 1960s San Francisco. This subpopulation has certain characteristics that may not be seen in more recently collected data or in varying geographic locations. Due to this, even our cross validation tests may not have been entirely representative of out-of-sample prediction. This may have biased us towards selecting a final model with a larger amount of terms in the formula. 


## Appendix for R Code

### A. Pre-Fitting 

```{r,eval=FALSE}

# Creating pairs plots, adjusting which variables we want in it
pairs(chdbirths[,c(1,2,3,14,8)],pch=16, cex=.3, col=adjustcolor("black",0.5))

# Create histogram of birthweight to view distribution
hist(chdbirths$wt,freq=FALSE, breaks="FD", col="gray90",
     xlab = "Healthy Male Birth Weight", main = "Histogram of Birth Weight")
# Overlay Gaussian approximation
curve(dnorm(x,mean=mean(chdbirths$wt),sd=sd(chdbirths$wt)),col="red",add=TRUE)


## Re-coding the data that doesn't make sense ordinally into categorical variables

# Mother and father ethnicity
births$meth[chdbirths$meth %in% c(0,1,2,3,4,5)] <- "Caucasian"
births$meth[chdbirths$meth == 6] <- "Mexican"
births$meth[chdbirths$meth == 7] <- "AA"
births$meth[chdbirths$meth == 8] <- "Asian"
births$meth[chdbirths$meth %in% c(9,10)] <- "Other" # grouping in Mixed 

births$feth[chdbirths$feth %in% c(0,1,2,3,4,5)] <- "Caucasian"
births$feth[chdbirths$feth == 6] <- "Mexican"
births$feth[chdbirths$feth == 7] <- "AA"
births$feth[chdbirths$feth == 8] <- "Asian"
births$feth[chdbirths$feth %in% c(9,10)] <- "Other" # grouping in Mixed 

# Mother and father education
births$med[chdbirths$med == 0] <- "elem"
births$med[chdbirths$med == 1] <- "mid"
births$med[chdbirths$med == 2] <- "hs"
births$med[chdbirths$med == 3] <- "hs+trade"
births$med[chdbirths$med == 4] <- "hs+some+col"
births$med[chdbirths$med == 5] <- "col"
births$med[chdbirths$med == 6] <- "trade"
births$med[chdbirths$med == 7] <- "hs+unclear"

births$fed[chdbirths$fed == 0] <- "elem"
births$fed[chdbirths$fed == 1] <- "mid"
births$fed[chdbirths$fed == 2] <- "hs"
births$fed[chdbirths$fed == 3] <- "hs+trade"
births$fed[chdbirths$fed == 4] <- "hs+some+col"
births$fed[chdbirths$fed == 5] <- "col"
births$fed[chdbirths$fed == 6] <- "trade"
births$fed[chdbirths$fed == 7] <- "hs+unclear"

# Marital Status
births$marital[chdbirths$marital == 1] <- "married"
# Grouping all other types because vast majority of obs. are married
births$marital[chdbirths$marital %in% c(0,2,3,4,5)] <- "other"

# Smoke
births$smoke[chdbirths$smoke == 0] <- "never"
births$smoke[chdbirths$smoke == 1] <- "smokesnow"
births$smoke[chdbirths$smoke == 2] <- "untilpreg"
births$smoke[chdbirths$smoke == 3] <- "usedto"

#Time, keep ordinal but change to more sensible order
births$time[chdbirths$time == 1] <- 8
births$time[chdbirths$time == 2] <- 7
births$time[chdbirths$time == 3] <- 6
births$time[chdbirths$time == 4] <- 5
births$time[chdbirths$time == 5] <- 4
births$time[chdbirths$time == 6] <- 3
births$time[chdbirths$time == 7] <- 2
births$time[chdbirths$time == 8] <- 1
births$time[chdbirths$time == 9] <- NA # only 5 observations, will be deleted

##


# Counting NA's in each variate
NAcount <- sapply(chdbirths, function(x) sum(is.na(x)))

# Dropping father height and weight since so many NA's 
drops <- c("fwt","fht")
births <- births[,!(names(births) %in% drops)]

# Using Amelia R package, display a graph of proportion of missing vals
missmap(births, col=c("yellow","midnightblue"),y.labels="",y.at="")

# Displaying differences between observations and imputed data
densityplot(imp_all) 
```

The following code was used to run the mice algorithm and create 5 imputed data sets. 
```{r,eval=FALSE}
#removing observations with NAs in variables that aren't income
birthsCompl <- births[complete.cases(births$marital, births$meth,
                                     births$med,births$feth,
                                     births$fed, births$smoke, births$time, births$number),]
# using the mice function to impute missing values over 5 iterations
set.seed(300)
imp_all <- mice(birthsCompl,m=5)
```
### B. Model Selection

This code pertains to our Automatic Model Selection in the report.  
```{r,eval=FALSE}
# Stepwise Model Selection
for (i in 1:5){ # run through each imputation mice returns 
  compldata <- complete(imp_all,i) # Completing data set over each imputation
  M0 <- lm(wt~1,data=compldata) # minimal model: intercept only
  
  # all main effects and interactions
  ##  Removed feth, meth, fed, med interactions because they produced NA's
  Mmax <- lm(wt ~ (.-feth - meth - fed - med)^2 + feth + meth + fed + med,data=compldata)
  Mstart <-lm(wt ~., data = compldata) # starting model for stepwise
  Mstep <- step(object= Mstart,
              scope=list(lower=M0, upper = Mmax),
              direction ="both", # stepwise direction 
              trace=FALSE)
  model <- paste("Ms",i, sep="") # dynamically name models with corresponding index
  assign(model,Mstep)
}

# Forward Model Selection
for (i in 1:5){ # run through each imputation mice returns 
  compldata <- complete(imp_all,i) # Completing data set over each imputation
  Mfwd <- step(object = M0,
              scope=list(lower=M0, upper = Mmax),
              direction ="forward", # forward direction 
              trace=FALSE)
  model <- paste("Mf",i, sep="") # dynamically name models with corresponding index
  assign(model,Mfwd)
}

# Backwards Model Selection
for (i in 1:5){ # run through each imputation mice returns
  compldata <- complete(imp_all,i) # Completing data set over each imputation
  Mback <- step(object = Mmax,
              scope=list(lower=M0, upper = Mmax),
              direction ="backward", # backwards direction 
              trace=FALSE)
  model <- paste("Mb",i, sep="") # dynamically name models with corresponding index
  assign(model,Mback)
}
```

### C. Manual Model Selection  

The following code pertains to our Manual Model Selection section of the report, and is used to generate the 3 tables of diagnostic statistics on the 15 models generated by automatic model selection.  

```{r,eval=FALSE}

# Calculate PRESS statistic for each of the 5 stepwise models, similar code for forward/backward
pressc1 <- sum((resid(Ms1)/(1-hatvalues(Ms1)))^2)
pressc2 <- sum((resid(Ms2)/(1-hatvalues(Ms2)))^2)
pressc3 <- sum((resid(Ms3)/(1-hatvalues(Ms3)))^2)
pressc4 <- sum((resid(Ms4)/(1-hatvalues(Ms4)))^2)
pressc5 <- sum((resid(Ms5)/(1-hatvalues(Ms5)))^2)


# Stepwise models summary of AIC, RSE, and PRESS statistic, similar code for forward/backward
step.matrix <- matrix(0, 3, 5) # initial matrix 
rownames(step.matrix) <- c("AIC", "RSE", "PRESS")
colnames(step.matrix) <- c("Ms1", "Ms2", "Ms3", "Ms4", "Ms5")

# Create vectors of AIC, RSE, PRESS for each model
aic.s <- round(c(AIC(Ms1), AIC(Ms2), AIC(Ms3), AIC(Ms4), AIC(Ms5)),2)
rse.s <- round(c(sigma(Ms1), sigma(Ms2), sigma(Ms3), sigma(Ms4), sigma(Ms5)),2)
press.s <- round(c(pressc1, pressc2, pressc3, pressc4, pressc5),2)
# Update matrix
step.matrix[1,] <- aic.s
step.matrix[2,] <- rse.s
step.matrix[3,] <- press.s
```

### D. Cross-Validation  

The following section of our code was used for our cross-validation of the forward and stepwise models. It pertains to the cross-validation section of our report.  


```{r,eval=FALSE}
# Fit stepwise model to imputations returned by mice 
fit.step <- with(data=imp_all, exp = lm(formula = wt ~ gestation + parity + meth + mage + 
    mht + mwt + marital + income + smoke + time + number + gestation:income + 
    gestation:number + parity:marital + mwt:income + mht:marital + 
    gestation:mage + gestation:mwt + gestation:mht + mage:time + 
    mage:number + smoke:number))


# Fit forward model to imputations returned by mice 
fit.fwd <- with(data=imp_all, expr = lm(formula = wt ~ gestation + 
    smoke + mht + meth + parity + number + mwt + time + gestation:number + 
    gestation:mwt + gestation:mht))


# Compare stepwise and forward models with Cross Validation
nreps <- 1000 # number of replications
ntot <- nrow(birthsCompl) # total number of observations
ntrain <- 0.8*ntot # size of training set is 80% of total 
ntest <- ntot - ntrain # size of test set is 20% of total 
mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
mspe2 <- rep(NA, nreps)

logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
m <-5 # number of imputations
 

for (ii in 1:nreps) {
  # randomly select training observations
  sample.ind <- sample(m,1) # randomly select which imputed data set to use
  M1.model <- fit.fwd$analyses[[sample.ind]]
  M2.model <- fit.step$analyses[[sample.ind]]
  train.ind <- sample(ntot, ntrain) # training observations
  # refit the models on the given imputed data and training indices 
  data_complete <- complete(imp_all,sample.ind)
  M1.cv <- update(M1.model, data = data_complete[train.ind,])
  M2.cv <- update(M2.model, data = data_complete[train.ind,])
  # out-of-sample residuals for both models
  # that is, testing data - predictions with training parameters
  M1.res <- data_complete$wt[-train.ind] -
    predict(M1.cv, newdata = data_complete[-train.ind, ])
  M2.res <- data_complete$wt[-train.ind] -
    predict(M2.cv, newdata = data_complete[-train.ind, ])
  # mean-square prediction errors
  mspe1[ii] <- mean(M1.res ^ 2)
  mspe2[ii] <- mean(M2.res ^ 2)
  # out-of-sample likelihood ratio
  M1.sigma <- sqrt(sum(resid(M1.cv) ^ 2) / ntrain) # MLE of sigma
  M2.sigma <- sqrt(sum(resid(M2.cv) ^ 2) / ntrain)
  # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
  logLambda[ii] <-
    sum(dnorm(
      M1.res,
      mean = 0,
      sd = M1.sigma,
      log = TRUE
    ))
  logLambda[ii] <- logLambda[ii] -
    sum(dnorm(
      M2.res,
      mean = 0,
      sd = M2.sigma,
      log = TRUE
    ))
}

# names of x axis for boxplot 
Mnames <- expression(M[FWD],M[STEP])

# Create boxplot
par(mfrow=c(1,2))
boxplot(list(sqrt(mspe1),sqrt(mspe2)), names = Mnames, ylab = expression(sqrt(MSPE)), cex = 0.7, col= c("springgreen4","skyblue"), cex.axis = 0.7, main = "RMSPE")

# logLambda histgram
hist(logLambda, breaks = 30, freq = FALSE, xlab = expression(Lambda^{test}), cex=0.7,
     main = "log of Lambda", col="gray93", cex.axis = 0.7)
abline(v=mean(logLambda),col="red",lwd=2)
```

### E. Model Diagnostics 

The following code looks at various residual plots for our two models. This code generates Figures 6-9.
```{r,eval=FALSE}
# We looked at residual plots for all 5 imputations for both models, but noticed that
## they were all very similar. As such, in the report and appendix we will only show 
## the plots generated for the 5th imputation. 

im <- 5 # let's just look at 5th imputation for both models

# Residuals for stepwise model
res.s <- resid(fit.step$analyses[[im]])
h.s <- hatvalues(fit.step$analyses[[im]])
res.stu.s <- res.s/sqrt(1-h.s) # studentized residuals

# Residuals for forward model
res.f <- resid(fit.fwd$analyses[[im]])
h.f <- hatvalues(fit.fwd$analyses[[im]])
res.stu.f <- res.f/sqrt(1-h.f) # studentized residuals

# Plotting residual plots for stepwise model, similar code for forwards model
par(mfrow=c(2,2))
plot(fit.step$analyses[[im]],cex=0.6,col=adjustcolor("skyblue4",0.5),
     cex.axis=0.7, pch=16)

# Plotting standardized residuals to see distribution, similar code for forwards model
par(mfrow=c(1,2))
sigma.hat.s <- sigma(fit.step$ana[[im]]) # pull RSE of 5th imputation

# Stepwise model studentized residual histogram
hist(res.stu.s/sigma.hat.s,breaks=40,freq=FALSE,cex.axis=0.8, 
     main = "Stepwise Model", xlab= "Studentized Residuals, standardized")
curve(dnorm(x),col="red",add=TRUE) # normal dist overlay

# Create influence vs. leverage plot with each model on the same graph
# Calculate measures of influence - Cook's distance for each model
D.step <- cooks.distance(fit.step$analyses[[im]])
D.fwd <- cooks.distance(fit.fwd$analyses[[im]])
# Calculate diagonals of hat matrix to get leverage measures for each model
h.step <- hatvalues(fit.step$analyses[[im]])
h.fwd <- hatvalues(fit.fwd$analyses[[im]])

inf <- which.is.max(D.step) # pull index of highest influence observation 
lev <- which.is.max(h.step) # pull index of highest leverage observation 


# Create scatterplot
# First add red points for stepwise model
plot(h.step,D.step, col=adjustcolor("firebrick",0.5), pch=16, 
     main = "Influence vs. Leverage Plot", xlab = "Leverage", 
     ylab= "Influence (Cook's Distance)", cex.axis=0.8)
# Now add blue points for forwards model
points(h.fwd,D.fwd,col=adjustcolor("navyblue",0.5),pch=18)
# Throw in a legend 
legend("topright", legend = c("Stepwise", "Forward"), pch = c(16,18),
       col = c("firebrick","navyblue"))
# Label highest influence point
text(x=h.step[inf], y = D.step[inf],
     labels = inf, pos=1, cex = 0.7, offs=0.4)
# Label highest leverage point
text(x=h.step[lev], y = D.step[lev],
     labels = lev, pos=c(1,1,1), cex = 0.7, offs=0.4)

# Pulling out highest influence point observation to take a look at 
complete(imp_all,5)[inf.ind,]

# Pulling out highest leverage points observation to take a look at 
complete(imp_all,5)[lev.ind,]

```

### F. Final Model Tuning

First, we re-ran automatic stepwise selection on the data set with the outlier removed.  
```{r,eval=FALSE}

for (i in 1:5){
  compldata <- complete(imp_all,i)[-246,] # Completing data set over each imputation
  M0.out <- lm(wt~1,data=compldata) # minimal model: intercept only
  
  # all main effects and interactions
  ##  Removed feth, meth, fed, med interactions because they produced NA's
  Mmax.out <- lm(wt ~ (.-feth - meth - fed - med)^2 + feth + meth + fed + med,data=compldata)
  Mstart.out <-lm(wt ~., data = compldata) # starting model for stepwise
  Mstep1 <- step(object= Mstart.out,
              scope=list(lower=M0.out, upper = Mmax.out),
              direction ="both",
              trace=FALSE)
  model <- paste("MsOut",i, sep="") # dynamically name models with corresponding index
  assign(model,Mstep1)
  print(Mstep1$call)
}
```

Then re-ran cross-validation on our tweaked stepwise model and the original one. The code belows shows the results of this, and how they turned out indistinguishable. 

```{r,fig.height=4.5}
options(warn=-1)



set.seed(331)
 # Cross-validation setup
 nreps <- 1000 # number of replications
 ntot <- nrow(birthsCompl) # total number of observations
 ntrain <- 0.8*ntot # size of training set is 80% of total 
 ntest <- ntot - ntrain # size of test set is 20% of total 
 mspe1 <- rep(NA, nreps) # sum-of-square errors for each CV replication
 mspe2 <- rep(NA, nreps)

 logLambda <- rep(NA, nreps) # log-likelihod ratio statistic for each replication
 m <-5 # number of imputations
 

 for (ii in 1:nreps) {
   
   # randomly select training observations
   sample.ind <- sample(m,1) # randomly select which imputed data set to use
   
   M1.model <- fit.stepOut$analyses[[sample.ind]]
   M2.model <- fit.step$analyses[[sample.ind]]
   train.ind <- sample(ntot, ntrain) # training observations
   # refit the models on the given imputed data and training indices 
   data_complete <- complete(imp_all,sample.ind)
   M1.cv <- update(M1.model, data = data_complete[train.ind,])
   M2.cv <- update(M2.model, data = data_complete[train.ind,])
   # out-of-sample residuals for both models
   # that is, testing data - predictions with training parameters
   M1.res <- data_complete$wt[-train.ind] -
     predict(M1.cv, newdata = data_complete[-train.ind, ])
   M2.res <- data_complete$wt[-train.ind] -
     predict(M2.cv, newdata = data_complete[-train.ind, ])
   # mean-square prediction errors
   mspe1[ii] <- mean(M1.res ^ 2)
   mspe2[ii] <- mean(M2.res ^ 2)
   # out-of-sample likelihood ratio
   M1.sigma <- sqrt(sum(resid(M1.cv) ^ 2) / ntrain) # MLE of sigma
   M2.sigma <- sqrt(sum(resid(M2.cv) ^ 2) / ntrain)
   # since res = y - pred, dnorm(y, pred, sd) = dnorm(res, 0, sd)
   logLambda[ii] <-
     sum(dnorm(
       M1.res,
       mean = 0,
       sd = M1.sigma,
       log = TRUE
     ))
   logLambda[ii] <- logLambda[ii] -
     sum(dnorm(
       M2.res,
       mean = 0,
       sd = M2.sigma,
       log = TRUE
     ))
 }
 
# names for x axis of boxplot 
Mnames <- expression(M[STEPout],M[STEP])

# create boxplot
par(mfrow=c(1,2))
boxplot(list(sqrt(mspe1),sqrt(mspe2)), names = Mnames, ylab = expression(sqrt(MSPE)), cex = 0.7, col= c("springgreen4","skyblue"), cex.axis = 0.7, main = "RMSPE")

# logLambda histogram
hist(logLambda, breaks = 30, freq = FALSE, xlab = expression(Lambda^{test}), cex=0.7,
     main = "log of Lambda", col="gray93", cex.axis = 0.7)
abline(v=mean(logLambda),col="red",lwd=2)


```



## Bibliography  

@journal{LBWarticle1,
  title = "The Effects Of Maternal Smoking, ..., On The Incidence Of Low Birth Weight",
  author = "Kleinmann, J and Madans, J.",
  month = "February",
  year = "2016",
  url = "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.820.7552&rep=rep1&type=pdf"
}

@journal{BWarticle2,
  title = "Quantitative Data Analysis of
    Multiple Factors Associated with
    Low Birth Weight in Bibb County,
    Georgia",
  author = "Jackson, H., Wei, Y., Chen, F.",
  year = {2008},
  url = "https://augusta.openrepository.com/augusta/bitstream/10675.2/610914/1/Jackson_2008_1_1.pdf",
}

